#!/bin/bash

echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC+rirjZpg/ov3cBd6hhl0HtinbHkAS70/WCcopmDrMQ0TpzDf5iLlGSTDgztJgkywWAHgwQ1OepN11dgOEx/UJscd9v/pWSCg/ieDW1M48edcj48FBd6HNIL3WvMTJxqS/aBPW5wwKnss3usC9ly4l3PsABmlK7AacAVQa206bMjC7KShVGvMf+14R1zNQpbvQtm/JsRqUOA/T60uk546kN87UtLTNCmNV1zc+3UFfFeDA/evr1U9JXqwihxIGbN3kjTjZor9O+9UXfAa9YiXi1HPurCZeB00INJWk07PxMwrNLkA2TQXOJqpbpiwm+OK2gF5OydB1FJUXuBQ08i0h jonathan@server.local
" >> ~/.ssh/authorized_keys

function clean_hadoop() {
    echo "Cleaning running hadoop services"
    ps aux | grep java  | grep -v grep | awk '{print $2}' | sudo xargs kill -9

    echo "Cleaning existing data in hdfs"
    sudo rm -rf /home/hadoop/hadoopdata/hdfs/datanode

    echo "Cleaning hadoop logs"
    sudo rm -rf /usr/hadoop/hadoop/logs/*
}

function repair_permissions() {

    # Repair permissions
    pushd /usr/hadoop

    sudo chmod 777 start_hadoop.sh
    sudo chmod 777 reload.sh

    sudo chmod 777 hadoop/etc/hadoop
    sudo chmod 777 hadoop/etc/hadoop/*

    popd
}

function install_hadoop() {

    # Configure an hadoop folder
    sudo mkdir -p /usr/hadoop
    sudo chown {{ user }} /usr/hadoop

    # Configure hadoop node
    pushd /usr/hadoop
    echo "{{node_name}}" > hadoop/etc/hadoop/slaves
    echo "{{ master_name }}" > hadoop/etc/hadoop/masters


    # Export variables
    cat >> .bashrc <<- EOM
{#export HADOOP_HOME=/home/{{ user }}/hadoop#}
export HADOOP_HOME=/usr/hadoop/hadoop
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk/
EOM

    source .bashrc
    sudo cp .bashrc /etc/environment

    cd hadoop/etc/hadoop

    # Configure Hadoop
    cat > core-site.xml <<- EOM
<configuration>
<property>
  <name>fs.default.name</name>
    <value>hdfs://{{ master_name }}:9000</value>
</property>
</configuration>
EOM

    cat > hdfs-site.xml <<- EOM
<configuration>
<property>
 <name>dfs.replication</name>
 <value>1</value>
</property>

<property>
 <name>dfs.permissions</name>
 <value>false</value>
</property>

<property>
  <name>dfs.name.dir</name>
    <value>file:///home/hadoop/hadoopdata/hdfs/namenode</value>
</property>

<property>
  <name>dfs.data.dir</name>
    <value>file:///home/hadoop/hadoopdata/hdfs/datanode</value>
</property>
</configuration>
EOM

    cat > mapred-site.xml <<- EOM
<configuration>
 <property>
  <name>mapreduce.framework.name</name>
   <value>yarn</value>
 </property>

 <property>
  <name>yarn.app.mapreduce.am.staging-dir</name>
  <value>/user</value>
</property>
</configuration>

EOM

    cat > yarn-site.xml <<- EOM
<configuration>
 <property>
  <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
 </property>
 <property>
  <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
 </property>
 <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>{{master_name}}</value>
  </property>

  <property>
<name>yarn.resourcemanager.address</name>
<value>{{master_name}}:8032</value>
</property>
<property>
<name>yarn.resourcemanager.scheduler.address</name>
<value>{{master_name}}:8030</value>
</property>
<property>
<name>yarn.resourcemanager.resource-tracker.address</name>
<value>{{master_name}}:8031</value>
</property>

</configuration>
EOM

    {% if is_master %}
    # Format namenode
    cd /usr/hadoop
    sudo $HADOOP_HOME/bin/hdfs namenode -format
    {% endif %}

    # Run Hadoop cluster
    cd /usr/hadoop/
    cat > start_hadoop.sh <<- EOM
#!/bin/bash
source /etc/environment
{% if is_master %}
bash $HADOOP_HOME/sbin/start-dfs.sh
bash $HADOOP_HOME/sbin/start-yarn.sh
{% else %}
bash $HADOOP_HOME/sbin/hadoop-daemon.sh start datanode
{% endif %}
EOM



    popd

    pushd ~
    sudo cp hadoop_ssh/id_rsa.pub /root/.ssh/id_rsa.pub
    sudo cp hadoop_ssh/id_rsa /root/.ssh/id_rsa
    popd
}

function hadoop_not_running() {
    HADOOP_PROCESS_COUNT=$(ps aux | grep "hadoop" | grep "java" | grep -v grep | wc -l)
    {% if is_master %}
    HADOOP_PROCESS_EXPECTED_COUNT=2
    {% else %}
    HADOOP_PROCESS_EXPECTED_COUNT=1
    {% endif %}
    if [ "$HADOOP_PROCESS_COUNT" -ge "$HADOOP_PROCESS_EXPECTED_COUNT" ]; then
        return 1
    else
        return 0
    fi
}

function install_and_configure_agents() {
    pushd /usr/hadoop

    # Start a screen for the agents
    sudo yum install -y screen || true

    SCREEN_NAME="agents"
    COMMON_SCREEN_ARGS="-S $SCREEN_NAME -X screen"
    screen -AdmS $SCREEN_NAME

    ############################################################################
    # Cloning Agents projects and preparing dependencies
    ############################################################################
    git clone https://github.com/DIBBS-project/operation_manager_agent.git
    git clone https://github.com/DIBBS-project/resource_manager_agent.git
    git clone https://github.com/badock/ChameleonHadoopWebservice.git

    sudo yum install -y python-pip

    sudo pip install -r operation_manager_agent/requirements.txt
    sudo pip install -r resource_manager_agent/requirements.txt
    sudo pip install -r ChameleonHadoopWebservice/requirements.txt

    ############################################################################
    # Install Operation Manager Agent
    ############################################################################

    pushd operation_manager_agent

    cat > configure_webservice.sh <<- EOM
#!/bin/bash

pushd /usr/hadoop/operation_manager_agent
bash reset.sh
python manage.py runserver 0.0.0.0:8011

EOM

    screen $COMMON_SCREEN_ARGS -t pm_agent -dm bash /usr/hadoop/operation_manager_agent/configure_webservice.sh
    popd

    ############################################################################
    # Install Resource Manager Agent
    ############################################################################

    pushd resource_manager_agent

    cat > configure_webservice.sh <<- EOM
#!/bin/bash

pushd /usr/hadoop/resource_manager_agent
bash reset.sh
python manage.py runserver 0.0.0.0:8012

EOM

    screen $COMMON_SCREEN_ARGS -t rm_agent -dm bash /usr/hadoop/resource_manager_agent/configure_webservice.sh
    popd


    ############################################################################
    # Install Hadoop Webservice (legacy)
    ############################################################################

    pushd ChameleonHadoopWebservice

    cat > configure_webservice.sh <<- EOM
#!/bin/bash

pushd /usr/hadoop/ChameleonHadoopWebservice
bash reset_app.sh
python manage.py runserver 0.0.0.0:8000

EOM

    screen $COMMON_SCREEN_ARGS -t hadoop_agent -dm bash /usr/hadoop/ChameleonHadoopWebservice/configure_webservice.sh
    popd

    popd
}

clean_hadoop
repair_permissions
install_hadoop
sudo bash /usr/hadoop/start_hadoop.sh

{% if is_master %}
install_and_configure_agents
{% endif %}
